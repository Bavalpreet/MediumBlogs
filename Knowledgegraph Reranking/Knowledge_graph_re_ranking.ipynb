{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TlCfac3Jpb9",
        "outputId": "198e56b3-5149-43fc-f2a8-651756d9f1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rank-bm25, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rank-bm25-0.2.2 torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install rank-bm25 sentence-transformers torch torch-geometric numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BM25 vs GNN"
      ],
      "metadata": {
        "id": "N313NmSe1FJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n",
        "\n",
        "# Step 1: Synthetic Corpus and Query\n",
        "corpus = [\n",
        "    \"The theory of relativity was developed by Albert Einstein in 1915.\",\n",
        "    \"Einstein's work on general relativity revolutionized physics.\",\n",
        "    \"Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg.\",\n",
        "    \"Special relativity describes the behavior of objects moving at high speeds.\",\n",
        "    \"The history of physics includes major discoveries by Newton and Einstein.\"\n",
        "]\n",
        "query = \"What is the theory of relativity?\"\n",
        "\n",
        "# Synthetic relevance labels (1 = relevant, 0 = less relevant)\n",
        "relevance_labels = {0: 1, 3: 1, 1: 1, 4: 0, 2: 0}  # Docs 0, 3, 1 relevant; Docs 4, 2 less relevant\n",
        "\n",
        "# Step 2: Initial Retrieval with BM25\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "tokenized_query = query.lower().split()\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "k = 4\n",
        "top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
        "initial_ranking = [(idx, corpus[idx], bm25_scores[idx]) for idx in top_k_indices]\n",
        "print(\"Initial BM25 Ranking:\")\n",
        "for idx, doc, score in initial_ranking:\n",
        "    print(f\"Doc {idx}: {doc} (Score: {score:.4f})\")\n",
        "\n",
        "# Step 3: Encoding Documents and Query\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "document_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=False)\n",
        "query_embedding = model.encode([query], convert_to_tensor=True, show_progress_bar=False)[0]\n",
        "initial_scores = cosine_similarity(query_embedding.unsqueeze(0).cpu().numpy(),\n",
        "                                  document_embeddings.cpu().numpy())[0]\n",
        "print(\"\\nInitial Cosine Similarity Scores:\")\n",
        "for idx, score in enumerate(initial_scores):\n",
        "    print(f\"Doc {idx}: {score:.4f}\")\n",
        "\n",
        "# Step 4: Graph Construction\n",
        "document_embeddings_np = document_embeddings.cpu().numpy()\n",
        "selected_embeddings_np = document_embeddings_np[top_k_indices].copy()\n",
        "print(\"\\nSelected embeddings strides:\", selected_embeddings_np.strides)\n",
        "similarity_matrix = cosine_similarity(selected_embeddings_np)\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "for i in range(k):\n",
        "    print([f\"{similarity_matrix[i, j]:.4f}\" for j in range(k)])\n",
        "\n",
        "# Use k-NN (2 neighbors) for edge construction\n",
        "edge_index = []\n",
        "edge_weight = []\n",
        "added_pairs = set()\n",
        "for i in range(k):\n",
        "    sim_scores = similarity_matrix[i].copy()\n",
        "    sim_scores[i] = -1  # Exclude self\n",
        "    top_neighbors = np.argsort(sim_scores)[::-1][:2]  # Top 2 neighbors\n",
        "    for neighbor in top_neighbors:\n",
        "        pair = tuple(sorted([i, neighbor]))\n",
        "        if pair not in added_pairs:\n",
        "            edge_index.append([i, neighbor])\n",
        "            edge_index.append([neighbor, i])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            added_pairs.add(pair)\n",
        "\n",
        "if not edge_index:\n",
        "    print(\"Warning: No edges formed. Using dummy edge.\")\n",
        "    edge_index = torch.tensor([[0, 0]], dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor([1.0], dtype=torch.float)\n",
        "else:\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "print(\"\\nEdges formed:\", edge_index.t().tolist())\n",
        "\n",
        "# Node features: Combine document embeddings with query relevance\n",
        "node_features = []\n",
        "for idx in top_k_indices:\n",
        "    doc_query_feature = document_embeddings[idx] * query_embedding\n",
        "    node_features.append(doc_query_feature.cpu().numpy())\n",
        "node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "# Step 5: Graph Neural Network for Re-ranking\n",
        "class GNNReRanker(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GNNReRanker, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.scorer = nn.Linear(hidden_dim + input_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.5)  # Increased dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.cat([x, data.x], dim=-1)\n",
        "        scores = self.scorer(x).squeeze(-1)\n",
        "        return scores\n",
        "\n",
        "# Initialize GNN and optimizer\n",
        "input_dim = node_features.shape[1]\n",
        "hidden_dim = 128\n",
        "gnn_model = GNNReRanker(input_dim, hidden_dim)\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01, weight_decay=1e-4)  # L2 regularization\n",
        "\n",
        "# Step 6: Train the GNN with early stopping\n",
        "gnn_model.train()\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    scores = gnn_model(graph_data)\n",
        "    loss = 0\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            idx_i, idx_j = top_k_indices[i], top_k_indices[j]\n",
        "            if idx_i in relevance_labels and idx_j in relevance_labels:\n",
        "                if relevance_labels[idx_i] > relevance_labels[idx_j]:\n",
        "                    loss += F.relu(scores[j] - scores[i] + 0.1)\n",
        "                elif relevance_labels[idx_j] > relevance_labels[idx_i]:\n",
        "                    loss += F.relu(scores[i] - scores[j] + 0.1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Step 7: Inference with Trained GNN\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gnn_scores = gnn_model(graph_data)\n",
        "\n",
        "# Step 8: Combine GNN Scores with Initial Scores\n",
        "bm25_top_k = torch.tensor([bm25_scores[idx] for idx in top_k_indices], dtype=torch.float)\n",
        "bm25_top_k = torch.sigmoid(bm25_top_k)  # Softer normalization\n",
        "gnn_scores = torch.sigmoid(gnn_scores)  # Softer normalization\n",
        "final_scores = 0.5 * bm25_top_k + 0.5 * gnn_scores  # Balanced weighting\n",
        "\n",
        "# Step 9: Final Re-ranking\n",
        "reranked_indices = torch.argsort(final_scores, descending=True)\n",
        "print(\"\\nFinal Re-ranked List:\")\n",
        "for rank, rerank_idx in enumerate(reranked_indices):\n",
        "    orig_idx = top_k_indices[rerank_idx]\n",
        "    print(f\"Rank {rank+1}: Doc {orig_idx}: {corpus[orig_idx]} (Final Score: {final_scores[rerank_idx]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB0jbcSpkOO5",
        "outputId": "be76f6f2-3b22-4eeb-bb88-5b58c6262268"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial BM25 Ranking:\n",
            "Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Score: 1.5514)\n",
            "Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Score: 0.4619)\n",
            "Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Score: 0.4619)\n",
            "Doc 2: Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg. (Score: 0.2055)\n",
            "\n",
            "Initial Cosine Similarity Scores:\n",
            "Doc 0: 0.7309\n",
            "Doc 1: 0.5700\n",
            "Doc 2: 0.2514\n",
            "Doc 3: 0.5889\n",
            "Doc 4: 0.4414\n",
            "\n",
            "Selected embeddings strides: (1536, 4)\n",
            "\n",
            "Similarity Matrix:\n",
            "['1.0000', '0.5990', '0.4308', '0.3947']\n",
            "['0.5990', '1.0000', '0.3185', '0.5143']\n",
            "['0.4308', '0.3185', '1.0000', '0.1448']\n",
            "['0.3947', '0.5143', '0.1448', '1.0000']\n",
            "\n",
            "Edges formed: [[0, 1], [1, 0], [0, 2], [2, 0], [1, 3], [3, 1], [2, 1], [1, 2], [3, 0], [0, 3]]\n",
            "Epoch 0, Loss: 0.3978\n",
            "Epoch 20, Loss: 0.0620\n",
            "Early stopping at epoch 35\n",
            "\n",
            "Final Re-ranked List:\n",
            "Rank 1: Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Final Score: 0.6760)\n",
            "Rank 2: Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Final Score: 0.5737)\n",
            "Rank 3: Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Final Score: 0.5530)\n",
            "Rank 4: Doc 2: Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg. (Final Score: 0.5169)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BM25 vs GNN+ GAR"
      ],
      "metadata": {
        "id": "CiqTrpxf1Ts-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Synthetic Corpus and Query\n",
        "corpus = [\n",
        "    \"The theory of relativity was developed by Albert Einstein in 1915.\",\n",
        "    \"Einstein's work on general relativity revolutionized physics.\",\n",
        "    \"Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg.\",\n",
        "    \"Special relativity describes the behavior of objects moving at high speeds.\",\n",
        "    \"The history of physics includes major discoveries by Newton and Einstein.\"\n",
        "]\n",
        "query = \"What is the theory of relativity?\"\n",
        "relevance_labels = {0: 1, 3: 1, 1: 1, 4: 0, 2: 0}\n",
        "k = 4\n",
        "\n",
        "# Step 2: Query Augmentation with T5\n",
        "def generate_augmented_queries(query, model, tokenizer, device='cpu', num_contexts=3):\n",
        "    input_text = f\"Generate keywords for: {query}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=50,\n",
        "        num_return_sequences=num_contexts,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    augmented_queries = [query]\n",
        "    for output in outputs:\n",
        "        context = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        augmented_queries.append(f\"{query} {context}\")\n",
        "    return augmented_queries\n",
        "\n",
        "# Initialize T5 model\n",
        "try:\n",
        "    t5_model_name = \"t5-base\"\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "except Exception:\n",
        "    t5_model_name = \"t5-small\"\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "t5_model = t5_model.to(device)\n",
        "augmented_queries = generate_augmented_queries(query, t5_model, t5_tokenizer, device)\n",
        "print(\"Augmented Queries:\")\n",
        "for aq in augmented_queries:\n",
        "    print(f\"- {aq}\")\n",
        "\n",
        "# Step 3: BM25 Retrieval with Augmented Queries and Hybrid Fallback\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "retrieved_indices = set()\n",
        "bm25_scores = np.zeros(len(corpus))\n",
        "\n",
        "for aq in augmented_queries:\n",
        "    tokenized_aq = aq.lower().split()\n",
        "    scores = bm25.get_scores(tokenized_aq)\n",
        "    top_indices = np.argsort(scores)[::-1][:k]\n",
        "    for idx in top_indices:\n",
        "        retrieved_indices.add(idx)\n",
        "        bm25_scores[idx] = max(bm25_scores[idx], scores[idx])\n",
        "\n",
        "# Hybrid retrieval\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "document_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=False)\n",
        "query_embedding = model.encode([query], convert_to_tensor=True, show_progress_bar=False)[0]\n",
        "cosine_scores = cosine_similarity(query_embedding.unsqueeze(0).cpu().numpy(),\n",
        "                                 document_embeddings.cpu().numpy())[0]\n",
        "print(\"\\nRaw BM25 and Cosine Scores for All Documents:\")\n",
        "for idx in range(len(corpus)):\n",
        "    print(f\"Doc {idx}: BM25={bm25_scores[idx]:.4f}, Cosine={cosine_scores[idx]:.4f}\")\n",
        "\n",
        "# Normalize and combine scores\n",
        "bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
        "cosine_norm = (cosine_scores - cosine_scores.min()) / (cosine_scores.max() - cosine_scores.min() + 1e-10)\n",
        "combined_scores = 0.5 * bm25_norm + 0.5 * cosine_norm\n",
        "top_k_indices = np.argsort(combined_scores)[::-1][:k]\n",
        "initial_ranking = [(idx, corpus[idx], combined_scores[idx], bm25_scores[idx], cosine_scores[idx]) for idx in top_k_indices]\n",
        "print(\"\\nInitial Combined Ranking (GAR + Cosine):\")\n",
        "for idx, doc, comb_score, bm25_score, cos_score in initial_ranking:\n",
        "    print(f\"Doc {idx}: {doc} (Combined: {comb_score:.4f}, BM25: {bm25_score:.4f}, Cosine: {cos_score:.4f})\")\n",
        "\n",
        "# Step 4: Graph Construction\n",
        "document_embeddings_np = document_embeddings.cpu().numpy()\n",
        "selected_embeddings_np = document_embeddings_np[top_k_indices].copy()\n",
        "print(\"\\nSelected embeddings strides:\", selected_embeddings_np.strides)\n",
        "similarity_matrix = cosine_similarity(selected_embeddings_np)\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "for i in range(k):\n",
        "    print([f\"{similarity_matrix[i, j]:.4f}\" for j in range(k)])\n",
        "\n",
        "# k-NN (2 neighbors) for edges\n",
        "edge_index = []\n",
        "edge_weight = []\n",
        "added_pairs = set()\n",
        "for i in range(k):\n",
        "    sim_scores = similarity_matrix[i].copy()\n",
        "    sim_scores[i] = -1\n",
        "    top_neighbors = np.argsort(sim_scores)[::-1][:2]\n",
        "    for neighbor in top_neighbors:\n",
        "        pair = tuple(sorted([i, neighbor]))\n",
        "        if pair not in added_pairs:\n",
        "            edge_index.append([i, neighbor])\n",
        "            edge_index.append([neighbor, i])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            added_pairs.add(pair)\n",
        "\n",
        "if not edge_index:\n",
        "    print(\"Warning: No edges formed. Using dummy edge.\")\n",
        "    edge_index = torch.tensor([[0, 0]], dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor([1.0], dtype=torch.float)\n",
        "else:\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "print(\"\\nEdges formed:\", edge_index.t().tolist())\n",
        "\n",
        "# Node features\n",
        "node_features = []\n",
        "for idx in top_k_indices:\n",
        "    doc_query_feature = document_embeddings[idx] * query_embedding\n",
        "    cosine_feature = cosine_scores[idx]\n",
        "    feature = np.concatenate([doc_query_feature.cpu().numpy(), [cosine_feature]])\n",
        "    node_features.append(feature)\n",
        "node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "# Step 5: GNN for Re-ranking\n",
        "class GNNReRanker(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GNNReRanker, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.scorer = nn.Linear(hidden_dim + input_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.cat([x, data.x], dim=-1)\n",
        "        scores = self.scorer(x).squeeze(-1)\n",
        "        return scores\n",
        "\n",
        "# Initialize GNN\n",
        "input_dim = node_features.shape[1]\n",
        "hidden_dim = 128\n",
        "gnn_model = GNNReRanker(input_dim, hidden_dim)\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01, weight_decay=1e-3)\n",
        "\n",
        "# Step 6: Train GNN\n",
        "gnn_model.train()\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "min_loss_threshold = 1e-4\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    scores = gnn_model(graph_data)\n",
        "    loss = 0\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            idx_i, idx_j = top_k_indices[i], top_k_indices[j]\n",
        "            if idx_i in relevance_labels and idx_j in relevance_labels:\n",
        "                if relevance_labels[idx_i] > relevance_labels[idx_j]:\n",
        "                    loss += F.relu(scores[j] - scores[i] + 0.2)  # Increased margin\n",
        "                elif relevance_labels[idx_j] > relevance_labels[idx_i]:\n",
        "                    loss += F.relu(scores[i] - scores[j] + 0.2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "    if loss.item() < best_loss and loss.item() > min_loss_threshold:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Step 7: Inference\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gnn_scores = gnn_model(graph_data)\n",
        "print(\"\\nRaw GNN Scores:\")\n",
        "for idx, score in zip(top_k_indices, gnn_scores):\n",
        "    print(f\"Doc {idx}: {score:.4f}\")\n",
        "\n",
        "# Step 8: Combine Scores\n",
        "bm25_top_k = torch.tensor([bm25_scores[idx] for idx in top_k_indices], dtype=torch.float)\n",
        "bm25_top_k = torch.sigmoid(bm25_top_k)\n",
        "gnn_scores = torch.sigmoid(gnn_scores)\n",
        "final_scores = 0.3 * bm25_top_k + 0.7 * gnn_scores\n",
        "print(\"\\nFinal Score Components:\")\n",
        "for idx, bm25_s, gnn_s, final_s in zip(top_k_indices, bm25_top_k, gnn_scores, final_scores):\n",
        "    print(f\"Doc {idx}: BM25_Sigmoid={bm25_s:.4f}, GNN_Sigmoid={gnn_s:.4f}, Final={final_s:.4f}\")\n",
        "\n",
        "# Step 9: Final Re-ranking\n",
        "reranked_indices = torch.argsort(final_scores, descending=True)\n",
        "print(\"\\nFinal Re-ranked List (GAR + Cosine + GNN):\")\n",
        "for rank, rerank_idx in enumerate(reranked_indices):\n",
        "    orig_idx = top_k_indices[rerank_idx]\n",
        "    print(f\"Rank {rank+1}: Doc {orig_idx}: {corpus[orig_idx]} (Final Score: {final_scores[rerank_idx]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obKqqbxa0aky",
        "outputId": "387b7e6f-7cc2-44dd-cb79-55c17b27b628"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented Queries:\n",
            "- What is the theory of relativity?\n",
            "- What is the theory of relativity? for: General keywords for: What is the theory of relativity?\n",
            "- What is the theory of relativity? for: for: Generate new keywords for: What is the theory of relativity?\n",
            "- What is the theory of relativity? for: for: Generate keywords for: What is the theory of relativity?\n",
            "\n",
            "Raw BM25 and Cosine Scores for All Documents:\n",
            "Doc 0: BM25=3.1028, Cosine=0.7309\n",
            "Doc 1: BM25=1.3053, Cosine=0.5700\n",
            "Doc 2: BM25=0.4109, Cosine=0.2514\n",
            "Doc 3: BM25=0.9238, Cosine=0.5889\n",
            "Doc 4: BM25=0.9238, Cosine=0.4414\n",
            "\n",
            "Initial Combined Ranking (GAR + Cosine):\n",
            "Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Combined: 1.0000, BM25: 3.1028, Cosine: 0.7309)\n",
            "Doc 1: Einstein's work on general relativity revolutionized physics. (Combined: 0.4983, BM25: 1.3053, Cosine: 0.5700)\n",
            "Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Combined: 0.4472, BM25: 0.9238, Cosine: 0.5889)\n",
            "Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Combined: 0.2933, BM25: 0.9238, Cosine: 0.4414)\n",
            "\n",
            "Selected embeddings strides: (1536, 4)\n",
            "\n",
            "Similarity Matrix:\n",
            "['1.0000', '0.6399', '0.4308', '0.5990']\n",
            "['0.6399', '1.0000', '0.3860', '0.6101']\n",
            "['0.4308', '0.3860', '1.0000', '0.3185']\n",
            "['0.5990', '0.6101', '0.3185', '1.0000']\n",
            "\n",
            "Edges formed: [[0, 1], [1, 0], [0, 3], [3, 0], [1, 3], [3, 1], [2, 0], [0, 2], [2, 1], [1, 2]]\n",
            "Epoch 0, Loss: 0.5978\n",
            "Epoch 20, Loss: 0.2470\n",
            "Early stopping at epoch 21\n",
            "\n",
            "Raw GNN Scores:\n",
            "Doc 0: 0.4363\n",
            "Doc 1: 0.3858\n",
            "Doc 3: 0.4036\n",
            "Doc 4: 0.2518\n",
            "\n",
            "Final Score Components:\n",
            "Doc 0: BM25_Sigmoid=0.9570, GNN_Sigmoid=0.6074, Final=0.7123\n",
            "Doc 1: BM25_Sigmoid=0.7867, GNN_Sigmoid=0.5953, Final=0.6527\n",
            "Doc 3: BM25_Sigmoid=0.7158, GNN_Sigmoid=0.5996, Final=0.6344\n",
            "Doc 4: BM25_Sigmoid=0.7158, GNN_Sigmoid=0.5626, Final=0.6086\n",
            "\n",
            "Final Re-ranked List (GAR + Cosine + GNN):\n",
            "Rank 1: Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Final Score: 0.7123)\n",
            "Rank 2: Doc 1: Einstein's work on general relativity revolutionized physics. (Final Score: 0.6527)\n",
            "Rank 3: Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Final Score: 0.6344)\n",
            "Rank 4: Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Final Score: 0.6086)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: here better generative models like gpt 4o series can be used with much refined prompt to see better effect of GAR. Here my aim is to show you how to do it, rest it can always be refined or improved."
      ],
      "metadata": {
        "id": "XRFeUJ2P42Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BM25 vs GAR"
      ],
      "metadata": {
        "id": "ZlXslg_A1ovj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required packages: pip install rank_bm25 sentence-transformers torch torch-geometric numpy sklearn\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Synthetic Corpus and Query\n",
        "corpus = [\n",
        "    \"The theory of relativity was developed by Albert Einstein in 1915.\",\n",
        "    \"Einstein's work on general relativity revolutionized physics.\",\n",
        "    \"Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg.\",\n",
        "    \"Special relativity describes the behavior of objects moving at high speeds.\",\n",
        "    \"The history of physics includes major discoveries by Newton and Einstein.\"\n",
        "]\n",
        "query = \"What is the theory of relativity?\"\n",
        "\n",
        "# Synthetic relevance labels (1 = relevant, 0 = less relevant)\n",
        "relevance_labels = {0: 1, 3: 1, 1: 1, 4: 0, 2: 0}  # Docs 0, 3, 1 relevant; Docs 4, 2 less relevant\n",
        "\n",
        "# Step 2: Initial Retrieval with BM25\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "tokenized_query = query.lower().split()\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "initial_k = 3  # Initial top-k\n",
        "top_k_indices = np.argsort(bm25_scores)[::-1][:initial_k]\n",
        "initial_candidates = set(top_k_indices)\n",
        "print(\"Initial BM25 Ranking:\")\n",
        "for idx in top_k_indices:\n",
        "    print(f\"Doc {idx}: {corpus[idx]} (Score: {bm25_scores[idx]:.4f})\")\n",
        "\n",
        "# Step 3: Encoding Documents and Query\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "document_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=False)\n",
        "query_embedding = model.encode([query], convert_to_tensor=True, show_progress_bar=False)[0]\n",
        "document_embeddings_np = document_embeddings.cpu().numpy()\n",
        "\n",
        "# Ensure both tensors are on the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "document_embeddings = document_embeddings.to(device)\n",
        "query_embedding = query_embedding.to(device)\n",
        "\n",
        "# Step 4: Pre-compute Corpus-Wide Similarity Graph\n",
        "corpus_similarity = cosine_similarity(document_embeddings_np)\n",
        "print(\"\\nPre-computed Similarity Matrix:\")\n",
        "for i in range(len(corpus)):\n",
        "    print([f\"{corpus_similarity[i, j]:.4f}\" for j in range(len(corpus))])\n",
        "\n",
        "# Step 5: GAR Iterative Re-ranking and Expansion\n",
        "max_candidates = 5  # Budget for total candidates\n",
        "num_iterations = 2  # Number of expansion iterations\n",
        "current_candidates = initial_candidates.copy()\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Select embeddings for current candidates\n",
        "    current_indices = list(current_candidates)\n",
        "    current_embeddings = document_embeddings[current_indices].cpu().numpy()  # Move to CPU for similarity\n",
        "\n",
        "    # Compute similarity within current candidates\n",
        "    current_similarity = cosine_similarity(current_embeddings)\n",
        "    edge_index = []\n",
        "    edge_weight = []\n",
        "    added_pairs = set()\n",
        "    k_neighbors = 2  # Number of neighbors per node\n",
        "    for i in range(len(current_indices)):\n",
        "        sim_scores = current_similarity[i].copy()\n",
        "        sim_scores[i] = -1  # Exclude self\n",
        "        top_neighbors = np.argsort(sim_scores)[::-1][:k_neighbors]\n",
        "        for neighbor in top_neighbors:\n",
        "            pair = tuple(sorted([i, neighbor]))\n",
        "            if pair not in added_pairs:\n",
        "                edge_index.append([i, neighbor])\n",
        "                edge_index.append([neighbor, i])\n",
        "                edge_weight.append(current_similarity[i, neighbor])\n",
        "                edge_weight.append(current_similarity[i, neighbor])\n",
        "                added_pairs.add(pair)\n",
        "\n",
        "    if not edge_index:\n",
        "        edge_index = torch.tensor([[i, j] for i in range(len(current_indices)) for j in range(len(current_indices)) if i != j], dtype=torch.long).t().contiguous()\n",
        "        edge_weight = torch.tensor([current_similarity[i, j] for i in range(len(current_indices)) for j in range(len(current_indices)) if i != j], dtype=torch.float)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "\n",
        "    # Node features: Combine document and query embeddings\n",
        "    node_features = torch.stack([document_embeddings[idx] * query_embedding for idx in current_indices], dim=0).to(device)\n",
        "    graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "    # Step 6: GNN Re-ranking\n",
        "    class GNNReRanker(nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim):\n",
        "            super(GNNReRanker, self).__init__()\n",
        "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "            self.scorer = nn.Linear(hidden_dim + input_dim, 1)\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        def forward(self, data):\n",
        "            x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
        "            x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
        "            x = self.dropout(x)\n",
        "            x = F.relu(self.conv2(x, edge_index, edge_weight))\n",
        "            x = self.dropout(x)\n",
        "            x = torch.cat([x, data.x], dim=-1)\n",
        "            scores = self.scorer(x).squeeze(-1)\n",
        "            return scores\n",
        "\n",
        "    input_dim = node_features.shape[1]\n",
        "    hidden_dim = 128\n",
        "    gnn_model = GNNReRanker(input_dim, hidden_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "    # Train GNN with early stopping\n",
        "    gnn_model.train()\n",
        "    best_loss = float('inf')\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()\n",
        "        scores = gnn_model(graph_data)\n",
        "        loss = 0\n",
        "        for i in range(len(current_indices)):\n",
        "            for j in range(i + 1, len(current_indices)):\n",
        "                idx_i, idx_j = current_indices[i], current_indices[j]\n",
        "                if idx_i in relevance_labels and idx_j in relevance_labels:\n",
        "                    if relevance_labels[idx_i] > relevance_labels[idx_j]:\n",
        "                        loss += F.relu(scores[j] - scores[i] + 0.1)\n",
        "                    elif relevance_labels[idx_j] > relevance_labels[idx_i]:\n",
        "                        loss += F.relu(scores[i] - scores[j] + 0.1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Iter {iteration}, Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "        if loss.item() < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    # Inference\n",
        "    gnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        gnn_scores = gnn_model(graph_data)\n",
        "\n",
        "    # Combine with BM25 scores\n",
        "    bm25_scores_current = torch.tensor([bm25_scores[idx] for idx in current_indices], dtype=torch.float).to(device)\n",
        "    bm25_scores_current = torch.sigmoid(bm25_scores_current)\n",
        "    gnn_scores = torch.sigmoid(gnn_scores)\n",
        "    current_scores = 0.5 * bm25_scores_current + 0.5 * gnn_scores\n",
        "\n",
        "    # Re-rank and expand\n",
        "    reranked_order = torch.argsort(current_scores, descending=True)\n",
        "    top_indices = [current_indices[i] for i in reranked_order[:2]]  # Top 2 for expansion\n",
        "\n",
        "    # Expand candidates using corpus similarity\n",
        "    new_candidates = set()\n",
        "    for idx in top_indices:\n",
        "        sim_scores = corpus_similarity[idx]\n",
        "        neighbor_indices = np.argsort(sim_scores)[::-1][1:3]  # Top 2 neighbors excluding self\n",
        "        for neighbor in neighbor_indices:\n",
        "            if len(current_candidates) < max_candidates and neighbor not in current_candidates:\n",
        "                new_candidates.add(neighbor)\n",
        "    current_candidates.update(new_candidates)\n",
        "    print(f\"Iter {iteration + 1} Candidates: {sorted(list(current_candidates))}\")\n",
        "\n",
        "# Step 7: Final Ranking\n",
        "final_indices = list(current_candidates)\n",
        "final_embeddings = document_embeddings[final_indices].cpu().numpy()\n",
        "final_similarity = cosine_similarity(final_embeddings)\n",
        "edge_index = []\n",
        "edge_weight = []\n",
        "added_pairs = set()\n",
        "for i in range(len(final_indices)):\n",
        "    sim_scores = final_similarity[i].copy()\n",
        "    sim_scores[i] = -1\n",
        "    top_neighbors = np.argsort(sim_scores)[::-1][:2]\n",
        "    for neighbor in top_neighbors:\n",
        "        pair = tuple(sorted([i, neighbor]))\n",
        "        if pair not in added_pairs:\n",
        "            edge_index.append([i, neighbor])\n",
        "            edge_index.append([neighbor, i])\n",
        "            edge_weight.append(final_similarity[i, neighbor])\n",
        "            edge_weight.append(final_similarity[i, neighbor])\n",
        "            added_pairs.add(pair)\n",
        "\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "node_features = torch.stack([document_embeddings[idx] * query_embedding for idx in final_indices], dim=0).to(device)\n",
        "final_graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    final_gnn_scores = gnn_model(final_graph)\n",
        "\n",
        "bm25_final = torch.tensor([bm25_scores[idx] for idx in final_indices], dtype=torch.float).to(device)\n",
        "bm25_final = torch.sigmoid(bm25_final)\n",
        "final_gnn_scores = torch.sigmoid(final_gnn_scores)\n",
        "final_scores = 0.5 * bm25_final + 0.5 * final_gnn_scores\n",
        "\n",
        "final_ranking = torch.argsort(final_scores, descending=True)\n",
        "print(\"\\nFinal GAR Re-ranked List:\")\n",
        "for rank, rerank_idx in enumerate(final_ranking):\n",
        "    orig_idx = final_indices[rerank_idx]\n",
        "    print(f\"Rank {rank+1}: Doc {orig_idx}: {corpus[orig_idx]} (Final Score: {final_scores[rerank_idx]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2koGzbeV4mdJ",
        "outputId": "5e77135c-9389-4756-809f-a5c83fbb5563"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial BM25 Ranking:\n",
            "Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Score: 1.5514)\n",
            "Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Score: 0.4619)\n",
            "Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Score: 0.4619)\n",
            "\n",
            "Pre-computed Similarity Matrix:\n",
            "['1.0000', '0.6399', '0.3947', '0.4308', '0.5990']\n",
            "['0.6399', '1.0000', '0.3441', '0.3860', '0.6101']\n",
            "['0.3947', '0.3441', '1.0000', '0.1448', '0.5143']\n",
            "['0.4308', '0.3860', '0.1448', '1.0000', '0.3185']\n",
            "['0.5990', '0.6101', '0.5143', '0.3185', '1.0000']\n",
            "Iter 0, Epoch 0, Loss: 0.1989\n",
            "Iter 0, Epoch 20, Loss: 0.0725\n",
            "Early stopping at epoch 25\n",
            "Iter 1 Candidates: [np.int64(0), np.int64(1), np.int64(3), np.int64(4)]\n",
            "Iter 1, Epoch 0, Loss: 0.3001\n",
            "Iter 1, Epoch 20, Loss: 0.0099\n",
            "Early stopping at epoch 20\n",
            "Iter 2 Candidates: [np.int64(0), np.int64(1), np.int64(3), np.int64(4)]\n",
            "\n",
            "Final GAR Re-ranked List:\n",
            "Rank 1: Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Final Score: 0.6836)\n",
            "Rank 2: Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Final Score: 0.5790)\n",
            "Rank 3: Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Final Score: 0.5630)\n",
            "Rank 4: Doc 1: Einstein's work on general relativity revolutionized physics. (Final Score: 0.5183)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BM25 vs GNN vs GAR + Cosine vs GAR + Cosine + GNN"
      ],
      "metadata": {
        "id": "Rvp6qs_X19MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Synthetic Corpus and Query\n",
        "corpus = [\n",
        "    \"The theory of relativity was developed by Albert Einstein in 1915.\",\n",
        "    \"Einstein's work on general relativity revolutionized physics.\",\n",
        "    \"Quantum mechanics emerged in the early 20th century with contributions from Planck and Heisenberg.\",\n",
        "    \"Special relativity describes the behavior of objects moving at high speeds.\",\n",
        "    \"The history of physics includes major discoveries by Newton and Einstein.\"\n",
        "]\n",
        "query = \"What is the theory of relativity?\"\n",
        "relevance_labels = {0: 1, 3: 1, 1: 1, 4: 0, 2: 0}\n",
        "k = 4\n",
        "\n",
        "# Step 2: Query Augmentation with T5\n",
        "def generate_augmented_queries(query, model, tokenizer, device='cpu', num_contexts=3):\n",
        "    input_text = f\"Generate keywords for: {query}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=50,\n",
        "        num_return_sequences=num_contexts,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    augmented_queries = [query]\n",
        "    for output in outputs:\n",
        "        context = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        augmented_queries.append(f\"{query} {context}\")\n",
        "    return augmented_queries\n",
        "\n",
        "# Initialize T5 model\n",
        "try:\n",
        "    t5_model_name = \"t5-base\"\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "except Exception:\n",
        "    t5_model_name = \"t5-small\"\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "t5_model = t5_model.to(device)\n",
        "augmented_queries = generate_augmented_queries(query, t5_model, t5_tokenizer, device)\n",
        "print(\"Augmented Queries:\")\n",
        "for aq in augmented_queries:\n",
        "    print(f\"- {aq}\")\n",
        "\n",
        "# Step 3: BM25 Retrieval with Augmented Queries and Hybrid Fallback\n",
        "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "retrieved_indices = set()\n",
        "bm25_scores = np.zeros(len(corpus))\n",
        "\n",
        "for aq in augmented_queries:\n",
        "    tokenized_aq = aq.lower().split()\n",
        "    scores = bm25.get_scores(tokenized_aq)\n",
        "    top_indices = np.argsort(scores)[::-1][:k]\n",
        "    for idx in top_indices:\n",
        "        retrieved_indices.add(idx)\n",
        "        bm25_scores[idx] = max(bm25_scores[idx], scores[idx])\n",
        "\n",
        "# Hybrid retrieval\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "document_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=False)\n",
        "query_embedding = model.encode([query], convert_to_tensor=True, show_progress_bar=False)[0]\n",
        "cosine_scores = cosine_similarity(query_embedding.unsqueeze(0).cpu().numpy(),\n",
        "                                 document_embeddings.cpu().numpy())[0]\n",
        "print(\"\\nRaw BM25 and Cosine Scores for All Documents:\")\n",
        "for idx in range(len(corpus)):\n",
        "    print(f\"Doc {idx}: BM25={bm25_scores[idx]:.4f}, Cosine={cosine_scores[idx]:.4f}\")\n",
        "\n",
        "# Normalize and combine scores\n",
        "bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-10)\n",
        "cosine_norm = (cosine_scores - cosine_scores.min()) / (cosine_scores.max() - cosine_scores.min() + 1e-10)\n",
        "combined_scores = 0.5 * bm25_norm + 0.5 * cosine_norm\n",
        "top_k_indices = np.argsort(combined_scores)[::-1][:k]\n",
        "initial_ranking = [(idx, corpus[idx], combined_scores[idx], bm25_scores[idx], cosine_scores[idx]) for idx in top_k_indices]\n",
        "print(\"\\nInitial Combined Ranking (GAR + Cosine):\")\n",
        "for idx, doc, comb_score, bm25_score, cos_score in initial_ranking:\n",
        "    print(f\"Doc {idx}: {doc} (Combined: {comb_score:.4f}, BM25: {bm25_score:.4f}, Cosine: {cos_score:.4f})\")\n",
        "\n",
        "# Step 4: Graph Construction\n",
        "document_embeddings_np = document_embeddings.cpu().numpy()\n",
        "selected_embeddings_np = document_embeddings_np[top_k_indices].copy()\n",
        "print(\"\\nSelected embeddings strides:\", selected_embeddings_np.strides)\n",
        "similarity_matrix = cosine_similarity(selected_embeddings_np)\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "for i in range(k):\n",
        "    print([f\"{similarity_matrix[i, j]:.4f}\" for j in range(k)])\n",
        "\n",
        "# k-NN (2 neighbors) for edges\n",
        "edge_index = []\n",
        "edge_weight = []\n",
        "added_pairs = set()\n",
        "for i in range(k):\n",
        "    sim_scores = similarity_matrix[i].copy()\n",
        "    sim_scores[i] = -1\n",
        "    top_neighbors = np.argsort(sim_scores)[::-1][:2]\n",
        "    for neighbor in top_neighbors:\n",
        "        pair = tuple(sorted([i, neighbor]))\n",
        "        if pair not in added_pairs:\n",
        "            edge_index.append([i, neighbor])\n",
        "            edge_index.append([neighbor, i])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            edge_weight.append(similarity_matrix[i, neighbor])\n",
        "            added_pairs.add(pair)\n",
        "\n",
        "if not edge_index:\n",
        "    print(\"Warning: No edges formed. Using dummy edge.\")\n",
        "    edge_index = torch.tensor([[0, 0]], dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor([1.0], dtype=torch.float)\n",
        "else:\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "print(\"\\nEdges formed:\", edge_index.t().tolist())\n",
        "\n",
        "# Node features\n",
        "node_features = []\n",
        "for idx in top_k_indices:\n",
        "    doc_query_feature = document_embeddings[idx] * query_embedding\n",
        "    cosine_feature = cosine_scores[idx]\n",
        "    feature = np.concatenate([doc_query_feature.cpu().numpy(), [cosine_feature]])\n",
        "    node_features.append(feature)\n",
        "node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weight)\n",
        "\n",
        "# Step 5: GNN for Re-ranking\n",
        "class GNNReRanker(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(GNNReRanker, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.scorer = nn.Linear(hidden_dim + input_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.cat([x, data.x], dim=-1)\n",
        "        scores = self.scorer(x).squeeze(-1)\n",
        "        return scores\n",
        "\n",
        "# Initialize GNN\n",
        "input_dim = node_features.shape[1]\n",
        "hidden_dim = 128\n",
        "gnn_model = GNNReRanker(input_dim, hidden_dim)\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01, weight_decay=1e-3)\n",
        "\n",
        "# Step 6: Train GNN\n",
        "gnn_model.train()\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "min_loss_threshold = 1e-4\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    scores = gnn_model(graph_data)\n",
        "    loss = 0\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            idx_i, idx_j = top_k_indices[i], top_k_indices[j]\n",
        "            if idx_i in relevance_labels and idx_j in relevance_labels:\n",
        "                if relevance_labels[idx_i] > relevance_labels[idx_j]:\n",
        "                    loss += F.relu(scores[j] - scores[i] + 0.2)  # Increased margin\n",
        "                elif relevance_labels[idx_j] > relevance_labels[idx_i]:\n",
        "                    loss += F.relu(scores[i] - scores[j] + 0.2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "    if loss.item() < best_loss and loss.item() > min_loss_threshold:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Step 7: Inference\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    gnn_scores = gnn_model(graph_data)\n",
        "print(\"\\nRaw GNN Scores:\")\n",
        "for idx, score in zip(top_k_indices, gnn_scores):\n",
        "    print(f\"Doc {idx}: {score:.4f}\")\n",
        "\n",
        "# Step 8: Combine Scores\n",
        "bm25_top_k = torch.tensor([bm25_scores[idx] for idx in top_k_indices], dtype=torch.float)\n",
        "bm25_top_k = torch.sigmoid(bm25_top_k)\n",
        "gnn_scores = torch.sigmoid(gnn_scores)\n",
        "final_scores = 0.3 * bm25_top_k + 0.7 * gnn_scores\n",
        "print(\"\\nFinal Score Components:\")\n",
        "for idx, bm25_s, gnn_s, final_s in zip(top_k_indices, bm25_top_k, gnn_scores, final_scores):\n",
        "    print(f\"Doc {idx}: BM25_Sigmoid={bm25_s:.4f}, GNN_Sigmoid={gnn_s:.4f}, Final={final_s:.4f}\")\n",
        "\n",
        "# Step 9: Final Re-ranking\n",
        "reranked_indices = torch.argsort(final_scores, descending=True)\n",
        "print(\"\\nFinal Re-ranked List (GAR + Cosine + GNN):\")\n",
        "for rank, rerank_idx in enumerate(reranked_indices):\n",
        "    orig_idx = top_k_indices[rerank_idx]\n",
        "    print(f\"Rank {rank+1}: Doc {orig_idx}: {corpus[orig_idx]} (Final Score: {final_scores[rerank_idx]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpDbmanb6T6_",
        "outputId": "e20c6347-bd9b-4aa1-d3bc-e47ce915976e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented Queries:\n",
            "- What is the theory of relativity?\n",
            "- What is the theory of relativity? for: for: What is the theory of relativity? Generate keyword generators for: What is the theory of relativity?\n",
            "- What is the theory of relativity? for: What are the keywords for: What is the theory of relativity?\n",
            "- What is the theory of relativity? for: I want to know a generic term for: What is the theory of relativity?\n",
            "\n",
            "Raw BM25 and Cosine Scores for All Documents:\n",
            "Doc 0: BM25=4.6543, Cosine=0.7309\n",
            "Doc 1: BM25=0.0000, Cosine=0.5700\n",
            "Doc 2: BM25=0.6164, Cosine=0.2514\n",
            "Doc 3: BM25=1.3857, Cosine=0.5889\n",
            "Doc 4: BM25=1.3857, Cosine=0.4414\n",
            "\n",
            "Initial Combined Ranking (GAR + Cosine):\n",
            "Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Combined: 1.0000, BM25: 4.6543, Cosine: 0.7309)\n",
            "Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Combined: 0.5008, BM25: 1.3857, Cosine: 0.5889)\n",
            "Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Combined: 0.3469, BM25: 1.3857, Cosine: 0.4414)\n",
            "Doc 1: Einstein's work on general relativity revolutionized physics. (Combined: 0.3322, BM25: 0.0000, Cosine: 0.5700)\n",
            "\n",
            "Selected embeddings strides: (1536, 4)\n",
            "\n",
            "Similarity Matrix:\n",
            "['1.0000', '0.4308', '0.5990', '0.6399']\n",
            "['0.4308', '1.0000', '0.3185', '0.3860']\n",
            "['0.5990', '0.3185', '1.0000', '0.6101']\n",
            "['0.6399', '0.3860', '0.6101', '1.0000']\n",
            "\n",
            "Edges formed: [[0, 3], [3, 0], [0, 2], [2, 0], [1, 0], [0, 1], [1, 3], [3, 1], [2, 3], [3, 2]]\n",
            "Epoch 0, Loss: 0.6261\n",
            "Epoch 20, Loss: 0.2344\n",
            "Early stopping at epoch 26\n",
            "\n",
            "Raw GNN Scores:\n",
            "Doc 0: 0.5887\n",
            "Doc 3: 0.5580\n",
            "Doc 4: 0.3646\n",
            "Doc 1: 0.5318\n",
            "\n",
            "Final Score Components:\n",
            "Doc 0: BM25_Sigmoid=0.9906, GNN_Sigmoid=0.6431, Final=0.7473\n",
            "Doc 3: BM25_Sigmoid=0.7999, GNN_Sigmoid=0.6360, Final=0.6852\n",
            "Doc 4: BM25_Sigmoid=0.7999, GNN_Sigmoid=0.5902, Final=0.6531\n",
            "Doc 1: BM25_Sigmoid=0.5000, GNN_Sigmoid=0.6299, Final=0.5909\n",
            "\n",
            "Final Re-ranked List (GAR + Cosine + GNN):\n",
            "Rank 1: Doc 0: The theory of relativity was developed by Albert Einstein in 1915. (Final Score: 0.7473)\n",
            "Rank 2: Doc 3: Special relativity describes the behavior of objects moving at high speeds. (Final Score: 0.6852)\n",
            "Rank 3: Doc 4: The history of physics includes major discoveries by Newton and Einstein. (Final Score: 0.6531)\n",
            "Rank 4: Doc 1: Einstein's work on general relativity revolutionized physics. (Final Score: 0.5909)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rewYpwQj6Zb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}